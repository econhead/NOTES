\documentclass[12pt,a4paper,fleqn]{article}

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{parskip}
\usepackage{amsmath, amssymb, graphicx}
\usepackage{tcolorbox}
\usepackage{fancyhdr}
\setlength{\headheight}{15.6pt}
\pagestyle{fancyplain}
\fancyhead[L]{Laxman Singh}
\fancyhead[R]{\today}
\usepackage{float}
\floatstyle{boxed}
\restylefloat{figure}
\graphicspath{ {/Users/econhead/NOTES/Probability/ProbClassNotes/17_11_2024/Figures} }
\author{Laxman Singh}
\date{\today}
\title{Linear Regression Contd.}

\begin{document}
\section{  \underline{ Linear Regression Contd. }  } 
  \(\theta_i\)'s: Parameters

 How to choose \(\theta_i\)'s? 

 Hypothesis: \(h_\theta(x)=\theta_1 x\)
 
 Parameters: \(\theta_1\)
 \begin{figure}[ht]
     \centering
     \includegraphics[scale=0.5]{2.png}
 \end{figure}
 
 Cost function:
  \begin{align*}
  J\left(\theta_0, \theta_1\right)=\frac{1}{2 m} \sum_{i=1}^m{\left(h_\theta\left(x^{(i)}\right)-y^{(i)}\right)}^2
  \end{align*}
 Goal: \(\min _{\theta_1} J\left(\theta_1\right)\)

 \(h_{\theta}(x)\): for fixed \(\theta_{1}\), this is a function of \(x\), and \(J(\theta_{1})\): function of the parameter \(\theta_{1}\).      
 \begin{figure}[ht]
     \centering
     \includegraphics[scale=0.7]{3.png}
 \end{figure}
 
  \begin{align*}
    J\left(\theta_1\right)&=\frac{1}{2 m} \sum_{i=1}^m{\left(h_\theta\left(x^{(i)}\right)-y^{(i)}\right)}^2\\
    &=\frac{1}{2 m} \sum_{i=1}^m{\left(\theta_1 x^{(i)}-y^{(i)}\right)}^2\\
    &=\frac{1}{2 m} \sum_{i=1}^m\left(0^2+0^2+0^2\right)=0
 \end{align*}

 \(h_{\theta}(x)\): for fixed \(\theta_{1}\), this is a function of \(x\), and \(J(\theta_{1})\): function of the parameter \(\theta_{1}\).      

 \begin{figure}[ht]
     \centering
     \includegraphics[scale=0.6]{4.png}
 \end{figure}

 Hypothesis: \(h_\theta(x)=\theta_0+\theta_1 x\)
 
 Parameters: \(\theta_0, \theta_1\) and  Goal: \(\min _{\theta_{0},\theta_1} J\left(\theta_{0},\theta_1\right)\)
 \begin{figure}[ht]
    \centering
    \includegraphics[scale=0.5]{1.png}
 \end{figure}
 
 Cost function:
 \[J\left(\theta_0, \theta_1\right)=\frac{1}{2 m} \sum_{i=1}^m{\left(h_\theta\left(x^{(i)}\right)-y^{(i)}\right)}^2\]
 


 \subsection{  \underline{ Gradient descent approach }  }  
 We have some function \(J\left(\theta_0, \theta_1\right)\)

 and we want to solve \(\min _{\substack{ \\\theta_1}} J\left(\theta_0, \theta_1\right)\)
 \begin{itemize}
    \item Start with some \(\theta_0, \theta_1\)
    \item Keep changing \(\theta_0, \theta_1\) to reduce \(J\left(\theta_0, \theta_1\right)\) until we hopefully end up at a minimum.
 \end{itemize}

 \begin{align*}
    & \theta_1:=\theta_1-\alpha \frac{\partial}{\partial \theta_1} J\left(\theta_1\right) \\
 \end{align*}
    
 If \(\alpha \) is too small, gradient descent can be slow.   
 \begin{figure}[ht]
    \centering
    \includegraphics[scale=0.3]{5.png}
 \end{figure}
 
 If alpha is too large, gradient descent can overshoot the minimum. It may fail to converge, or even diverge.
 
 \begin{figure}[H]
     \centering
     \includegraphics[scale=0.3]{6.png}
 \end{figure}

 Gradient descent can converge to a local minimum, even with the learning rate \(\alpha \)  fixed.
 \begin{align*}
 \theta_1:=\theta_1-\alpha \frac{\partial}{\partial \theta_1} J\left(\theta_1\right)
 \end{align*}
 As we approach a local minimum, gradient descent will automatically take smaller steps. So, no need to decrease \(\alpha \) over time.

 \textbf{Gradient Descent Algorithm} (repeat until convergence); 
 \begin{align*}
 \theta_j =\theta_j-\alpha\frac{\partial}{\partial \theta_{j}} J\left(\theta_0, \theta_1\right)
\end{align*}
 (for \(j=0\) and \(j=1\))

 \textbf{Linear Regression Model}
 \begin{align*}
 \begin{gathered}
 h_\theta(x)=\theta_0+\theta_1 x \\
 J\left(\theta_0, \theta_1\right)=\frac{1}{2 m} \sum_{i=1}^m{\left(h_\theta\left(x^{(i)}\right)-y^{(i)}\right)}^2
 \end{gathered}
 \end{align*}

 \subsubsection{\underline{Two Variable Model}} 
 \begin{align*}
    &\frac{\partial}{\partial \theta_j} J\left(\theta_0, \theta_1\right) =\frac{\partial}{\partial \theta_j} \frac{1}{2 m} \sum_{i=1}^m{\left(h_\theta\left(x^{(i)}\right)-y^{(i)}\right)}^2 \\ 
    & =\frac{\partial}{\partial \theta_j} \frac{1}{2 m} \sum_{i=1}^m{\left(\theta_0+\theta_1 x^{(i)}-y^{(i)}\right)}^2 \\
    &j=0: \frac{\partial}{\partial \theta_0} J\left(\theta_0, \theta_1\right)=\frac{1}{m} \sum_{i=1}^m\left(h_\theta\left(x^{(i)}\right)-y^{(i)}\right) \\ 
    &j=1: \frac{\partial}{\partial \theta_1} J\left(\theta_0, \theta_1\right)=\frac{1}{m} \sum_{i=1}^m\left(h_\theta\left(x^{(i)}\right)-y^{(i)}\right) \cdot x^{(i)}
 \end{align*}

 Classification
 \begin{itemize}
    \item Email: Spam/Not spam?
    \item Online Transactions: Fraudulent (Yes/No)?
    \item Tumor: Malignant / Benign?
    \item \(y \in \{0,1\} \), where
    \begin{itemize}
        \item 0: ``Negative class'' (e.g., benign tumor) and 
        \item 1: ``Positive class'' (e.g., malignant tumor).
    \end{itemize}
 \end{itemize}
 \begin{figure}[H]
     \centering
     \includegraphics[scale=0.7]{7.png}
 \end{figure}

 Threshold classifier output \(h_\theta(x)\) at 0.5:
\begin{itemize}
    \item  If \(h_\theta(x) \geq 0.5\), predict ``\(y=1\)''
    \item If \(h_\theta(x)<0.5\), predict  ``\(y=0\)''
\end{itemize}

\section{Logistic Regression Model}

We want \(0 \leq h_\theta(x) \leq 1\)
\begin{align*}
h_\theta(x)=g\left(\theta^T x\right)=\frac{1}{1+e^{-\theta^T x}}
\end{align*}
where \(g(z)=\frac{1}{1+e^{-z}}\) and it is known as Sigmoid function/Logistic function.

\begin{figure}[H]
    \centering
    \includegraphics[scale=0.4]{8.png}
\end{figure}

\textbf{Interpretation of Hypothesis Output} \(h_\theta(x)=\) estimated probability that \(y=1\) on input \(x\).

Example: If \(x=\left[\begin{array}{l}x_0 \\ x_1\end{array}\right]=\left[\begin{array}{c}1 \\ \text { tumor size }\end{array}\right]\)

\(h_\theta(x)=0.7\): tell patient that \(70 \% \) chance of tumor being malignant.

\(h_\theta(x)\) gives probability that \(y=1\), given \(x\), parameteized by \(\theta \).

 \subsection{Logistic Regression: Decision Boundary} 
 \textbf{Logistic Regression}
 \begin{align*}
 h_\theta(x)=g\left(\theta^T x\right) \quad ; \quad g(z)=\frac{1}{1+e^{-z}}
 \end{align*}
\begin{itemize}
    \item Suppose predict  ``\(y=1\)'' if \(h_\theta(x) \geq 0.5\), which happens when \(\theta^T x \geq 0\).
    \item Suppose predict ``\(y=0\)'' if \(h_\theta(x)<0.5\), which happens when \(\theta^T x<0\).
\end{itemize}
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.4]{8.png}
\end{figure}

 \subsubsection{Decision Boundary} 
 \begin{figure}[H]
     \centering
     \includegraphics[scale=0.7]{9.png}
 \end{figure}
 \begin{itemize}
    \item \(h_\theta(x)=g\left(\theta_0+\theta_1 x_1+\theta_2 x_2\right)\)
    \item Predict ``\(y=1\)'' if \(-3+x_1+x_2 \geq 0\)
 \end{itemize}
  \subsubsection{Non-Linear Decision Boundaries} 
 \begin{figure}[H]
     \centering
     \includegraphics[scale=0.5]{10.png}
 \end{figure}
 \begin{itemize}
    \item \(h_\theta(x)=g\left(\theta_0+\theta_1 x_1+\theta_2 x_2+\theta_3 x_1^2+\theta_4 x_2^2\right)\)
    \item Predict  ``\(y=1\)'' if \(-1+x_1^2+x_2^2 \geq 0\)
 \end{itemize}
  \subsection{Logistic regression: Cost Function} 
  \begin{itemize}
    \item Training set: \( \{ \left(x^{(1)}, y^{(1)}\right),\left(x^{(2)}, y^{(2)}\right), \ldots,\left(x^{(m)}, y^{(m)}\right)\} \)
    \item m examples
    \item \(x=\left[\begin{array}{c}x_0 \\ x_1 \\ \cdots \\ x_n\end{array}\right] \in \mathbb{R}^{n+1}, x_0=1, y \in \{0,1\} \)
    \item \(h_\theta(x)=\frac{1}{1+e^{-\theta^{\top} x}}\)
    \item How to choose parameters \(\theta \)?
  \end{itemize}

  \textbf{Logistic regression cost function}

\begin{align*}
\begin{gathered}
\operatorname{Cost}\left(h_\theta(x), y\right)= \begin{cases} \quad \log \left(h_\theta(x)\right), & \text { if } y=1 \\
-\log \left(1-h_\theta(x)\right), & \text { if } y=0\end{cases} \\
\operatorname{Cost}\left(h_\theta(x), y\right)=-y \log \left(h_\theta(x)\right)-(1-y) \log \left(1-h_\theta(x)\right)
\end{gathered}
\end{align*}
\begin{figure}[H]
    \centering
    \includegraphics[scale=0.5]{11.png}
\end{figure}
\begin{itemize}
    \item Cost \(=0\) if \(y=1, h_\theta(x)=1\). But as \(h_\theta(x) \rightarrow 0\), we have Cost \(\rightarrow \infty \).
    \item Captures intuition that if \(h_\theta(x)=0\), (predict \(\Pr(y=1 \mid x ; \theta)=0\)), but \(y=1\), we'll penalize learning algorithm by a very large cost.
\end{itemize}
\end{document} 